vad = 语音活动检测
毫秒级别完成这段信号是人在说话吗？还是只是噪音或者沉默？
声音信号：当你对着麦克风说话时，声波会被转换成电信号，然后通过ADC（模数转换器）采样成一系列数字。比如说，在16kHz采样率下，每秒钟会有16000个数字来表示声音的振幅。
基于能量的VAD --> 引入频域分析 为了看到每个频率成分的强度 来判断人声比例
使用工具：fft
==需要理解FFT==

[[FFT]]

分类：
基于能量的VAD
基于频谱特征的VAD算法
统计模型方法：使用贝叶斯定理
深度学习方法：音频信号转换成梅尔频谱图（二维的表示）横轴是时间，纵轴是频率
我们把这个梅尔频谱图输入到一个卷积神经网络或循环神经网络中。
卷积层：可以提取局部的时频特征，比如某个特定频率范围在短时间内的能量变化模式。
循环层：则可以捕捉时间上的依赖关系，比如语音通常不是孤立的瞬间现象，而是连续的音节和词汇。
激活函数：网络的最后一层通常是一个sigmoid或softmax激活函数，输出一个介于0到1之间的数字，表示"这一帧是语音的概率"。


VAD的时序平滑：避免抖动
问题描述：纯对每一帧独立判断都会带来一个问题：判断结果可能会频繁跳变，一会儿是语音一会儿不是，产生抖动。
方案1：时序平滑机制
一个简单的方法是使用滑动窗口和多数投票：我们不只看当前帧，而是看它前后几帧，如果大多数帧都判定为语音，那当前帧才算语音。
方案2：隐马尔可夫模型HMM或者在深度学习中使用CTC（Connectionist Temporal Classification）损失函数。
这些方法的核心思想是：状态之间的转换应该有一定的惯性，从语音状态切换到非语音状态需要一定的"证据积累"，不能瞬间切换。

工程优化：
1.延迟需要很低 只有几十毫秒 导致窗口不能太长 模型不能太复杂
2.噪声估计 需要动态调整阈值
3.收尾处理：语音的收尾能量弱 需要做衍生 （端点扩展）